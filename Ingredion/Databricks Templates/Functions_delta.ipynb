{"cells":[{"cell_type":"code","source":["import pyspark.sql\nfrom pyspark.sql.functions import udf\nimport pyspark.sql.functions as F\nimport pyspark.sql.types as T\nfrom pyspark.sql.functions import col, when, lit, coalesce,count \nfrom pyspark.sql.types import ArrayType, StringType, MapType, StructField, StructType, FloatType\nfrom pyspark.sql.window import Window\nimport sys\nfrom functools import reduce\nfrom typing import Dict, List, Any\nfrom datetime import datetime\nimport pkg_resources\nimport os, glob\nimport importlib\nimport logging\nimport pkgutil\nimport re"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4e294225-cc61-42e8-888c-d456a28eca60"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# this UDF is to fetch time from measure value and concatenate to reportdate\ndef reportdate_shifttime( reportdate, time):\n  reportdate = (str(reportdate)+\" \" + str(time))\n  return reportdate\nspark.udf.register(\"reportdate_shifttime\", reportdate_shifttime)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2f2af0c6-5a20-4901-8b61-7e62f8420473"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["#Function to get values from Json input file and sets default value to parameter if mandatory\ndef default_inputs(jsonval, defaultval):\n  try:\n    jsonval= inputs[jsonval] #True if filename column is needed. i.e. name of file from which record comes\n  except:\n    jsonval = defaultval\n  return jsonval"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e355b251-44d5-4cbb-a99c-73ff9a55bf0e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Stage: Replace char function is to remove any unexpected charater from data in stage notebook\ndef replaceChar(character, df: pyspark.sql.DataFrame) -> pyspark.sql.DataFrame:\n  charReplace = udf(lambda x: x.replace(character,'') if x is not None else x)\n  for column in df.schema.fields:\n    if isinstance(column.dataType, StringType):\n      df=df.withColumn(column.name,charReplace(column.name)) \n  return df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e7b943b6-ad5f-4655-95cd-544c79d44fd4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def date_func(date):\n  date_partition = \"Region=na/year=\"+date[0:4]+\"/month=\"+date[5:7]+\"/date=\"+date[8:10]+\"/\"\n  return date_partition "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fb6721f2-17cb-4bad-a5e9-c1430faa8fd4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def align_and_union(df1: pyspark.sql.DataFrame, df2: pyspark.sql.DataFrame,non_align_columns:list) -> pyspark.sql.DataFrame:\n    # union requries that df columns are aligned. this function aligns and unions by sorting the columns\n    return df1.select(sorted([colname for colname in list(set(df1.columns) - set(non_align_columns))])+non_align_columns).\\\n        union(df2.select(sorted([colname for colname in list(set(df2.columns) - set(non_align_columns))])+non_align_columns))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Function to union dataframes","showTitle":true,"inputWidgets":{},"nuid":"afadc6b5-1503-4654-8d7c-3bc12fbb8103"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def convert_to_list(list):\n  list_item = list.replace(\" \", \"\")\n  item_list = []\n  for i in list_item.split(','):\n    item_list.append(i)\n  return item_list"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"985d06bc-2739-410d-8504-5ce962135897"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def create_table_dataframe(table) -> pyspark.sql.DataFrame:\n  tabledf= spark.sql(\"select * from \" + table).alias(table.split(\".\")[1]) \n  return tabledf\n\n# def create_sql_dataframe(table) -> pyspark.sql.DataFrame:\n#   tabledf = spark.read.jdbc(url=jdbcUrl, table=table, properties=connectionProperties)\n#   return tabledf\n\ndef create_csv_dataframe(file_loc,delimiter ) -> pyspark.sql.DataFrame:\n  tabledf = spark.read.format(\"csv\").option(\"header\",True).option(\"inferSchema\", True)\\\n.option(\"delimiter\", delimiter).load(file_loc)\n  return tabledf\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7116197d-345c-4e67-9ca0-939c150d4546"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def create_sql_dataframe(table, source_sql_conn) -> pyspark.sql.DataFrame:\n  with open(\"/dbfs/mnt/deltajobinputs/\"+source_sql_conn, 'r') as file:\n    data = file.read()\n  connObject = json.loads(data)\n\n  conn: Dict[str, Dict[str, Any]] = connObject[\"SQLserver\"]  \n\n  jdbcHostname= conn[\"jdbcHostname\"]\n  jdbcPort= conn[\"jdbcPort\"]\n  jdbcDatabase= conn[\"jdbcDatabase\"]\n  connectionProperties= conn[\"connectionProperties\"]\n\n  # Create the JDBC URL without passing in the user and password parameters.\n  jdbcUrl = \"jdbc:sqlserver://{0}:{1};database={2}\".format(jdbcHostname,jdbcPort,jdbcDatabase)\n  tabledf = spark.read.jdbc(url=jdbcUrl, table=table, properties=connectionProperties)\n  return tabledf"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d4084db3-55d4-4714-a88a-a55bfd30c379"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-3263144130620027&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span><span class=\"ansi-green-fg\">def</span> create_sql_dataframe<span class=\"ansi-blue-fg\">(</span>table<span class=\"ansi-blue-fg\">,</span> source_sql_conn<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">-&gt;</span> pyspark<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">.</span>DataFrame<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span>   <span class=\"ansi-green-fg\">with</span> open<span class=\"ansi-blue-fg\">(</span>source_sql_conn<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#39;r&#39;</span><span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">as</span> file<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span>     data <span class=\"ansi-blue-fg\">=</span> file<span class=\"ansi-blue-fg\">.</span>read<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span>   connObject <span class=\"ansi-blue-fg\">=</span> json<span class=\"ansi-blue-fg\">.</span>loads<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> \n\n<span class=\"ansi-red-fg\">NameError</span>: name &#39;pyspark&#39; is not defined</div>","errorSummary":"<span class=\"ansi-red-fg\">NameError</span>: name &#39;pyspark&#39; is not defined","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-3263144130620027&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span><span class=\"ansi-green-fg\">def</span> create_sql_dataframe<span class=\"ansi-blue-fg\">(</span>table<span class=\"ansi-blue-fg\">,</span> source_sql_conn<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">-&gt;</span> pyspark<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">.</span>DataFrame<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span>   <span class=\"ansi-green-fg\">with</span> open<span class=\"ansi-blue-fg\">(</span>source_sql_conn<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#39;r&#39;</span><span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">as</span> file<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span>     data <span class=\"ansi-blue-fg\">=</span> file<span class=\"ansi-blue-fg\">.</span>read<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span>   connObject <span class=\"ansi-blue-fg\">=</span> json<span class=\"ansi-blue-fg\">.</span>loads<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> \n\n<span class=\"ansi-red-fg\">NameError</span>: name &#39;pyspark&#39; is not defined</div>"]}}],"execution_count":0},{"cell_type":"code","source":["def dumpsqlserver(df,url,table,mode, truncate,properties):  \n  df.write.mode(mode).option(\"truncate\",truncate).jdbc(url=url, table= table, mode =mode, properties = properties)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3d026bf1-d21f-425a-a5cc-c02cac4e3d46"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def extract(sources: Dict[str, Any]) -> Dict[str, pyspark.sql.DataFrame]:\n  inputs: Dict[str, pyspark.sql.DataFrame] = {}\n  for alias, properties in sources.items():\n    if properties[\"type\"] == \"table\":\n      df_input = create_table_dataframe(properties[\"source\"])      \n    elif properties[\"type\"] == \"query\":\n      df_input = spark.sql(properties[\"source\"])\n    elif properties[\"type\"] == \"sqltable\":\n      try:\n        sqlconnection = properties[\"sqlconnection\"]\n      except:\n        sqlconnection = sqlconnectionFile        \n      df_input =  create_sql_dataframe(properties[\"source\"], sqlconnection )     \n    elif properties[\"type\"] == \"csv\":\n      df_input = create_csv_dataframe(properties[\"source\"], properties[\"delimiter\"])\n    elif properties[\"type\"] == \"dataframe\":\n      df_input = properties[\"source\"]\n    inputs[alias] = df_input.alias(alias)\n  return inputs"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5610c368-4236-4be3-9a01-583102b5ace4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def transform( df_joined: pyspark.sql.DataFrame) -> pyspark.sql.DataFrame:\n    \"\"\"Transform the joined dataframe and return the target dataframe to be loaded into the DB\n    \"\"\"\n    columns = [mapping[\"source\"].alias(mapping.get(\"target\")) for mapping in target_mappings]\n    df_target = df_joined.select(columns)    \n    return df_target\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"838fcd7e-00c8-464e-a30c-93fdf1450c70"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def input(sources, joins, target_mappings):\n  inputs: Dict[str, pyspark.sql.DataFrame] = extract(sources)\n  #get the plantID\n  df_joined = join(inputs).distinct()  \n  df_target = transform(df_joined)\n  return df_target"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"17cc71f6-0410-495a-b406-c786cbb737f4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def join(inputs: Dict[str, pyspark.sql.DataFrameReader]) -> pyspark.sql.DataFrame: \n  # set base dataframe\n  source_alias = joins[0][\"source\"]\n  df_joined: pyspark.sql.DataFrame = inputs[source_alias]\n  # loop over join conditions and join dfs\n  for join_op in joins[1:]:\n    df_joined = df_joined.join(inputs[join_op[\"source\"]],\n                                       join_op.get(\"conditions\"),\n                                       how=join_op.get(\"type\", \"inner\")\n                                       )\n  return df_joined"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0b77cb58-9656-42bb-bab8-bb7945ca38ba"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#updated 10/22/2020 added ` for columns name where space is in column names\ndef convert_datatype(schema,inputdf,tableAppend,tableName,dateFormat,tsFormat,no_stage_table='N')-> pyspark.sql.DataFrame:\n    from pyspark.sql.types import IntegerType,DateType,TimestampType,DoubleType,LongType\n    inputColumns=set(inputdf.columns)\n    outputColumns=[]\n    \n    for column in schema.fields:\n      if (tableAppend=='Y') & (no_stage_table=='N'):\n        oldColumnName=tableName+'~'+column.name\n      elif no_stage_table=='Y':\n        oldColumnName=column.name\n        column.name=column.name.replace(tableName+'~','')\n      else:\n        oldColumnName=column.name\n      \n      if (no_stage_table=='N') & (oldColumnName not in inputColumns):\n          oldColumnName=column.name\n          if oldColumnName not in inputColumns:\n              inputdf=inputdf.drop(oldColumnName)\n              continue\n      if isinstance(column.dataType, StringType) :\n         inputdf=inputdf.withColumn(column.name, F.col(\"`\"+oldColumnName+\"`\").cast(\"string\"))\n       \n      if isinstance(column.dataType, IntegerType) :\n         inputdf=inputdf.withColumn(column.name, F.col(\"`\"+oldColumnName+\"`\").cast(\"integer\"))\n          \n      if isinstance(column.dataType, LongType) :\n         inputdf=inputdf.withColumn(column.name, F.col(\"`\"+oldColumnName+\"`\").cast(T.LongType()))\n         \n      if isinstance(column.dataType, DateType):\n         inputdf =inputdf.withColumn(column.name, F.to_date(F.col(\"`\"+oldColumnName+\"`\").cast(\"string\"),dateFormat))\n\n      if isinstance(column.dataType, TimestampType):\n         inputdf =inputdf.withColumn(column.name, F.to_timestamp(F.col(\"`\"+oldColumnName+\"`\"),tsFormat))\n  \n      if isinstance(column.dataType, DoubleType):\n         inputdf =inputdf.withColumn(column.name, F.col(\"`\"+oldColumnName+\"`\").cast(\"double\"))\n\n      if isinstance(column.dataType, FloatType):\n         inputdf =inputdf.withColumn(column.name, F.col(\"`\"+oldColumnName+\"`\").cast(\"float\"))\n      \n      #if stage column type is decimal and with (scale, precision) eg. decimal(10,2)\n      if str(column.dataType).startswith('DecimalType'):\n         inputdf =inputdf.withColumn(column.name, F.col(\"`\"+oldColumnName+\"`\").astype(str(column.dataType).replace(\"Type\",\"\")))\n          \n      if isinstance(column.dataType, StructType):\n        for arr in column.dataType.fields:\n          inputdf = inputdf.withColumn(column.name, F.explode(F.array(F.col(\"`\"+column.name+\".\"+arr.name +\"`\"))))\n          \n      if isinstance(column.dataType, ArrayType):\n        for arr in column.dataType.fields:\n          inputdf = inputdf.withColumn(column.name, F.explode(F.col(\"`\" +column.name+\".\"+arr.name +\"`\")))\n          \n      if (column.name!=oldColumnName):\n              inputdf=inputdf.drop(oldColumnName) \n      \n      outputColumns.append(\"`\"+column.name+\"`\")\n    inputdf=inputdf.select(outputColumns)\n    return inputdf"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7377f437-3ac9-4c70-8f25-32af8072a2da"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# replace invalid character(s) space, ,;{}()\\n\\t= in column names with '_'\ndef getalias(df_incoming):\n  for col in df_incoming.columns:\n    orig_col = col\n    col = col.strip().replace(\" \", \"_\")\n    for ch in [\",\", \";\", \"{\", \"}\", \"(\", \")\", \"\\n\", \"\\t\", \"=\", \".\"]:\n      col = col.replace(ch, \"\") \n    df_incoming = df_incoming.withColumnRenamed(orig_col, col )\n  return df_incoming"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"77ae1f8f-39cf-4895-8a5f-b818f85b69bb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def add_partition(partition,df) -> pyspark.sql.DataFrame:\n  partitions=partition.split(\"/\")\n  incoming_partition = []\n  for column in partitions:\n      incoming_partition.append(column.split(\"=\")[0])\n      df=df.withColumn(column.split(\"=\")[0], F.lit(column.split(\"=\")[1]))\n  return df,incoming_partition"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e8acaffb-cd12-4027-84ec-31c2407e1ba4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def getpartition(partition):\n  partitions=partition.split(\"/\")\n  outpartition = []\n  for column in partitions:\n      outpartition.append(column.split(\"=\")[0])\n  return outpartition"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e9253503-ad6c-46e2-8d7f-1f6ad24dd812"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def align_columns(df_source_raw, df_target_raw, add_missing=False, ignore_columns=[],non_align_columns=[]):\n    if add_missing:\n        # add NULL columns to align the df\n        df_source = df_source_raw\n        for column in df_target_raw.columns:\n            if column not in df_source.columns and column not in ignore_columns:\n                df_source = df_source.withColumn(column, F.lit(None).cast(df_target_raw.schema[column].dataType))\n    else:\n        df_source = df_source_raw\n\n    # return only shared columns\n    shared_existing_columns:List[str] = list(\n        set(df_source.columns).\\\n        intersection(set(df_target_raw.columns).\\\n        union(set(ignore_columns)))\n    )\n    df_source = df_source.select(*shared_existing_columns+non_align_columns)\n\n    return df_source"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e0d476bf-7956-4a05-bb8f-bc5be32b7b57"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def add_missing_columns(schema, inputdf):\n  # add NULL columns if columns are missing in incoming and present in stage\n  for column in schema:\n    if column.name not in inputdf.columns:\n      inputdf = inputdf.withColumn(column.name, F.lit(None).cast(column.dataType))  \n  return inputdf"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d2519021-ee9c-4d71-9bd4-06e22e29c463"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def readcsv(inferSchema, delimiter,quote_char,escape_char,multiline, raw):\n  if quote_char != 'N' and escape_char == 'N':\n    df_incoming = spark.read.format(\"csv\").option(\"header\", True).option(\"inferSchema\",inferSchema).option(\"delimiter\",delimiter).option(\"quote\", quote_char).load(raw)\n  elif quote_char == 'N' and escape_char != 'N' and multiline.lower() == \"true\":\n    df_incoming = spark.read.format(\"csv\").option(\"header\", True).option(\"inferSchema\",inferSchema).option(\"delimiter\",delimiter).option(\"multiline\", True).option(\"escape\", escape_char).load(raw)\n  elif quote_char == 'N' and escape_char != 'N' and multiline.lower() == \"false\":\n    df_incoming = spark.read.format(\"csv\").option(\"header\", True).option(\"inferSchema\",inferSchema).option(\"delimiter\",delimiter).option(\"escape\", escape_char).load(raw)\n  elif quote_char != 'N' and escape_char != 'N' and multiline.lower() == \"false\":\n    df_incoming = spark.read.format(\"csv\").option(\"header\", True).option(\"inferSchema\",inferSchema).option(\"delimiter\",delimiter).option(\"quote\", quote_char).option(\"escape\", escape_char).load(raw)\n  elif quote_char != 'N' and escape_char != 'N' and multiline.lower() == \"true\":\n    df_incoming = spark.read.format(\"csv\").option(\"header\", True).option(\"inferSchema\",inferSchema).option(\"delimiter\",delimiter).option(\"quote\", quote_char).option(\"multiline\", True).option(\"escape\", escape_char).load(raw) \n  elif(quote_char == 'N' and escape_char == 'N'):\n    df_incoming = spark.read.format(\"csv\").option(\"header\", True).option(\"inferSchema\",inferSchema).option(\"delimiter\",delimiter).load(raw)\n  return df_incoming"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"14c5fbdd-0e2e-409b-8fdd-3eb2fc002de5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def file_name(filepath): \n  path_list = filepath.split(\"/\")\n  return path_list[len(path_list)-1] "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"853a66bc-ab77-44f3-b596-57b4e940c7c4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def pre_stage(df_incoming, tableAppend,tableName, dateFormat, tsFormat):\n  df_incoming = getalias(df_incoming)\n  no_stage_table='N'\n\n  if replace_char !='N' :\n    df_incoming = replaceChar(replace_char, df_incoming)     \n\n  if len(out_partition)!=0:  \n    df_incoming,stg_partition = add_partition(out_partition,df_incoming)\n  else:\n    stg_partition=[]\n  try :\n    df_existing_stage = spark.sql(\"SELECT * FROM deltastage.\"+tableName)\n    schema=df_existing_stage.schema\n  except:\n    no_stage_table='Y'\n    schema=df_incoming.schema    \n\n  df_prestage=convert_datatype(schema,df_incoming,tableAppend=tableAppend,tableName=tableName,dateFormat=dateFormat, tsFormat=tsFormat,no_stage_table=no_stage_table)\n\n  df_prestage = add_missing_columns(schema, df_prestage)\n  if filename.lower() == \"true\":\n    get_file_name = udf(file_name, StringType())\n\n    df_prestage = df_prestage.withColumn(\"filename\", F.regexp_replace(get_file_name(F.input_file_name()),\"%20\",\" \"))\n\n  \n  #This is to replace extra spaces from table name mostly for Excel sheet names\n  tableName=tableName.replace(\" \", \"\")\n  \n  if len(stg_partition) != 0:  \n    df_prestage.write.format(\"delta\").mode(\"overwrite\").partitionBy(stg_partition).option(\"path\",stage).\\\n            saveAsTable(\"deltastage.\"+tableName.replace(\" \",\"\"))\n  else:\n    df_prestage.write.format(\"delta\").mode(\"overwrite\").option(\"path\",stage).saveAsTable(\"deltastage.\"+tableName.replace(\" \",\"\"))\n  \n  spark.sql(\"refresh table deltastage.`\" + tableName.replace(\" \",\"\")+ \"`\")\n\n  return df_prestage"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d4e0b413-82f8-4e7a-995e-b09f482e2edd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def upsert_dataframe(df_existing,df_insert,df_update,primary_key:Dict[str,Any]):\n    # join by primary key\n    df_update_joined = (df_existing.alias(\"existing_inactive\")).join(df_update.alias(\"new_update\"),\n    [\n        (F.col(f\"existing_inactive.{key}\")==F.col(f\"new_update.{key}\")) \n        # &\n        # (~F.isnull(F.col(f\"incoming.{key}\"))) \n        for key in primary_key.keys()\n    ],\n    how=\"leftouter\")\n    # we want to update, so populate the missing columns with existing data\n    missing_cols_update = set(df_existing.columns)-set(df_update.columns)\n    df_update_joined = df_update_joined.select(\"existing_inactive.*\",*[F.col(f\"new_update.{col}\") for col in missing_cols_update])\n    # add blank columns for any missing columns in the insert\n    missing_cols_insert = set(df_existing.columns)-set(df_insert.columns)\n    df_insert_joined = df_insert\n    for column in missing_cols_insert:\n        df_insert_joined = df_insert_joined.withColumn(column,F.lit(None).cast(df_existing.schema[column].dataType))\n\n    # add blank columns for any missing columns in the update\n    missing_cols_update = set(df_insert_joined.columns)-set(df_update_joined.columns)\n    for column in missing_cols_update:\n        df_update_joined = df_update_joined.withColumn(column,F.lit(None).cast(df_insert_joined.schema[column].dataType))\n\n    return align_and_union(df_insert_joined,df_update_joined)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"572ca80a-15cf-46e8-840e-86aebebed16c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def get_max_value(df, column, default=0):\n    max_value = df.select(F.max(F.col(column).cast(\"integer\")).alias(\"MAX\")).limit(1).collect()[0].MAX\n    if (max_value is None):\n        return default\n    else:\n        return max_value"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"922a2b77-388b-4a23-a22b-8befa3eb9938"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def df_zipwithindex(df, offset=1, col_name=\"rowId\"):\n    '''\n        Enumerates dataframe rows is native order, like rdd.ZipWithIndex(), but on a dataframe \n        and preserves a schema\n\n        :param df: source dataframe\n        :param offset: adjustment to zipWithIndex()'s index\n        :param colName: name of the index column\n    '''\n\n    new_schema = StructType(\n                    [StructField(col_name,T.LongType(),True)]        # new added field in front\n                    + df.schema.fields                            # previous schema\n                )\n\n    zipped_rdd = df.rdd.zipWithIndex()\n\n    new_rdd = zipped_rdd.map(lambda args: ([args[1] + offset] + list(args[0])))\n\n    return new_rdd.toDF(new_schema)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bf979638-33f0-4268-bce8-99ce64dca8f0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def fill_auto_increment(\n        df_existing: pyspark.sql.DataFrame,\n        df_new: pyspark.sql.DataFrame,\n        autoincrement_column: str):\n    # add primary key from existing to new by shared business key\n    # returns: df_new with a new primary key column\n    #           for new entries, returns an autoincrement value\n    try:\n      max_id = get_max_value(df_existing,autoincrement_column,0)+1\n    except:\n      max_id=0\n      \n    df_combined = df_new.orderBy(autoincrement_column)\n    df_combined = df_zipwithindex(df_combined,offset=max_id)\n    df_combined = df_combined.withColumn(autoincrement_column,\n        F.when( F.col(autoincrement_column).isNull(), \n                F.col(\"rowId\"))\\\n                .otherwise(F.col(autoincrement_column))).drop(\"rowId\")\n    return df_combined"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"576799ee-e8d4-48c6-b9b7-aa25146a6f69"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def get_latest_row(df, business_key, sort_key):\n  descorderby=[]\n  for sort in sort_key:\n    descorderby.append(F.desc(sort))\n  \n  df = df.select(F.row_number().over(Window.partitionBy(business_key).orderBy(descorderby)).alias(\"row_num\"),\"*\" )\\\n  .where(\"row_num == 1\").drop(\"row_num\")\n  return df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7105462f-c156-4fad-8354-e2e73612ebf5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def find_delta(df_existing, df_incoming,business_key, primary_key,non_align_columns=[]) -> pyspark.sql.DataFrame:\n  exist_key=\"\"\n  #getting latest record when duplicate found in incoming\n  try:\n    df_incoming_raw = get_latest_row(df_incoming, business_key, sort_key)\n  except:  \n    df_incoming_raw = df_incoming.withColumn(\"curr_row_flg\", F.lit('Y'))\n  \n  \n  df_existing_raw = align_columns(df_existing,df_incoming_raw,ignore_columns=primary_key.keys(),non_align_columns=non_align_columns,add_missing=True)\n  \n  join_condition = [\n        F.lower(F.coalesce(df_incoming_raw[business_key_column].cast(\"string\"), F.lit(''))) == F.lower(F.coalesce(df_existing_raw[business_key_column].cast(\"string\"), F.lit('')))\n        for business_key_column in business_key]\n  if len(join_condition) != 0:\n    df_merged =df_incoming_raw.alias('incoming').join(df_existing_raw.alias('existing'), join_condition, 'leftouter').distinct()\n  \n  \n    #New records in incremental\n  df_insert = df_merged\n  for business_key_col in business_key:\n      df_insert = df_insert.filter(col(\"existing.\"+business_key_col).isNull())\n  new_insert = df_insert.select(\"incoming.*\").withColumn(\"curr_row_flg\", F.lit('Y')).distinct()\n  if  len(primary_key) != 0:\n    if set(business_key) != set(primary_key.keys()):\n    # add empty column for primary key\n      for primary_key_col, primary_key_type in primary_key.items():\n        new_insert = new_insert.withColumn(\n            primary_key_col,\n            F.lit(None).cast(primary_key_type)).distinct() \n  \n    \n  #Incremental Matching records with Initial load\n  df_update = df_merged\n  get_diff = df_existing_raw.columns\n  common_key=[]\n  uncommon_key=[]\n  try:\n    for non in business_key+non_align_columns+list(primary_key):\n      get_diff.remove(non)\n      common_key.append(\"existing.\"+non)\n  except:\n    print(non + \" key not present\")\n  \n  for non_key in get_diff:\n    uncommon_key.append(\"incoming.\"+non_key)\n    \n  \n  delt_cols=[]\n  for i in get_diff:\n    if i not in partition_keys:\n      delt_cols.append(\"(incoming.`\"+i+ \"`!= existing.`\"+i+\"`)\")\n      delt_cols_diff = \" or \".join(delt_cols)\n  \n  \n  for business_key_col in business_key:\n      df_update = df_update.filter(col(\"existing.\"+business_key_col).isNotNull())\\\n      .where(delt_cols_diff) \n  if  len(primary_key) != 0:    \n    if set(business_key) != set(primary_key.keys()):\n    # add empty column for primary key\n      for primary_key_col, primary_key_type in primary_key.items():\n        if exist_key ==\"\":\n          exist_key = \"existing.\"+primary_key_col\n        else:\n          exist_key = exist_key + \",\"+\"existing.\"+primary_key_col\n          uncommon_key.append(exist_key)\n    new_update = df_update.select(common_key+uncommon_key).withColumn(\"curr_row_flg\", F.lit('Y')).distinct()\n  else:\n    new_update = df_update.select(common_key+uncommon_key).withColumn(\"curr_row_flg\", F.lit('Y')).distinct()\n          \n  #Initial load records which are updated in incremental \n  existing_inactive = df_update.select(\"existing.*\").withColumn(\"curr_row_flg\", F.lit('Y')).distinct()\n  \n  if hour_partition !='True':\n  #Records which don't have any update in incremental\n    df_existing_raw = df_existing_raw.withColumn(\"curr_row_flg\", F.lit('Y')).distinct()\n  else:\n    df_existing_raw = df_existing_raw.withColumn(\"curr_row_flg\", F.lit('Y'))\n    \n  existing_active = df_existing_raw.select(sorted([colname for colname in df_existing_raw.columns]))\\\n  .subtract(existing_inactive.select(sorted([colname for colname in existing_inactive.columns]))\\\n           ).distinct()\n  \n  return new_insert,new_update,existing_inactive,existing_active"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"94bc703e-cd81-46b2-b7fe-16d6458cc5c9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def get_param_value(input_param, index):  \n  param = input_param.split(\";\")[index].split(\"=\",1)[1]\n  return param"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d5eb7088-20b3-44be-8191-ba4905f4d0f4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def dumpxml(df_stage, tableName, path, rootTag, rowTag,mode):  \n  df_stage.repartition(1).write.mode(mode).format(\"com.databricks.spark.xml\").partitionBy(partition_keys).option(\"rootTag\", rootTag).option(\"rowTag\", rowTag).save(path)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"45ab8759-1d1d-4ff7-94f1-caa314be5899"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def unpivot_fields(df, column0,column1 ):  \n  pivot_cols = []\n  id_cols =[]\n  for i in df.columns:\n    df = df.withColumn(i, F.col(i).cast(\"string\")) \n\n  cnt = 0  \n  for i in df.columns:\n    if re.search(\"........-....-....-....-............\", i):      \n      cnt = cnt+1\n      colnm = \"'\"+i+\"'\"\n      cols = \"`\"+i+\"`\"\n      pivot_cols.append(colnm)\n      pivot_cols.append(cols)\n    else:\n      colnm = i\n      id_cols.append(colnm)\n      \n  stack_str = \",\".join(pivot_cols)\n  id_cols.append(\"stack(\"+str(cnt)+\",\"+stack_str+\")\")  \n  \n  unpivot = df.selectExpr(id_cols)\\\n            .withColumnRenamed(\"col0\",column0)\\\n            .withColumnRenamed(\"col1\",column1)\n  \n  return unpivot\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1e882a19-03dd-4a98-97a9-3c2f54669660"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def Loadtodatabricks(df, mode, partition_keys, transform_path, outputDB, outputTable,outputFileformat ):\n  if partition_keys != []:\n    if outputFileformat.lower() == \"orc\":\n      df.write.mode(mode).partitionBy(partition_keys).format(\"orc\").option(\"path\", transform_path).saveAsTable(outputDB+\".\"+ outputTable)\n    elif outputFileformat.lower() == \"delta\":\n      df.write.mode(mode).partitionBy(partition_keys).format(\"delta\").option(\"path\",  transform_path).saveAsTable(outputDB+\".\"+ outputTable)\n    else:\n      df.write.mode(mode).partitionBy(partition_keys).option(\"path\",  transform_path).saveAsTable(outputDB+\".\"+ outputTable)\n  else:\n    if outputFileformat.lower() == \"orc\":\n      df.write.format(\"orc\").mode(mode).partitionBy(partition_keys).option(\"path\", transform_path).saveAsTable(outputDB+\".\"+ outputTable)\n    elif outputFileformat.lower() == \"delta\":\n      df.write.format(\"delta\").mode(mode).partitionBy(partition_keys).option(\"path\", transform_path).saveAsTable(outputDB+\".\"+ outputTable)      \n    else:\n      df.write.mode(mode).partitionBy(partition_keys).option(\"path\",  transform_path).saveAsTable(outputDB+\".\"+ outputTable)\n  sqlContext.sql(\"refresh table \"+outputDB+\".\"+ outputTable)\n  return "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5c03c5ce-79a3-4d5e-99c3-1d598244cbf6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def add_datepartition(df, date_column, dateFormat):  \n  if date_column in [\"current_date\", \"DateUpdated\"]:\n    df=df.withColumn(\"year\", F.year(F.lit(_timestamp)))\n    df=df.withColumn(\"month\", F.month(F.lit(_timestamp)))\n    df=df.withColumn(\"date\", F.dayofmonth(F.lit(_timestamp)))\n  elif date_column == 'N' :\n    df =df\n  else:\n    df.drop('year','month','date')\n    if dateFormat != \"\":\n      df = df.withColumn(\"date_column\", F.to_date(F.col(date_column).cast(\"string\"), dateFormat))        \n      df=df.withColumn(\"year\", F.year(F.col(\"date_column\")))\n      df=df.withColumn(\"month\", F.month(F.col(\"date_column\")))\n      df=df.withColumn(\"date\", F.dayofmonth(F.col(\"date_column\")))\n      df=df.drop(\"date_column\")\n    else:\n      df=df.withColumn(\"year\", F.year(F.col(\"date_column\")))\n      df=df.withColumn(\"month\", F.month(F.col(\"date_column\")))\n      df=df.withColumn(\"date\", F.dayofmonth(F.col(\"date_column\")))\n  return df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ef199380-badd-430e-b15f-755591098ec4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["\"\"\"\"this function generates required conditions for merge: 1. mergejoin: merge join condition based on business key 2. whenMatchedUpdateset: when business key columns matched then replace existing data values with incoming data values. DateInserted will remain same as existing. 3. when business key columns do not match then insert incoming data to existing table. \"\"\"\n\ndef merge_inputs(df_existing,business_key ) :  \n  mergejoin = \"\"\n  tablecols = df_existing.columns\n  for bk in business_key:\n    mergejoin = mergejoin + \"existing.\"+bk+\" = incoming.\"+bk\n    if bk != business_key[len(business_key)-1]:\n      mergejoin =  mergejoin + \" and \"\n\n    tablecols.remove(bk)\n\n  tablecols.remove(\"DateInserted\")\n  whenMatchedUpdateset:Dict[str, Any] = {}\n  for col in tablecols:\n    whenMatchedUpdateset[\"existing.`\"+col +\"`\"] =\"incoming.`\"+col+\"`\"\n\n  whenNotMatchedUpdateset:Dict[str, Any] = {}\n  for excol in  df_existing.columns:\n    whenNotMatchedUpdateset[\"`\"+excol+\"`\"] = \"incoming.`\"+excol+\"`\"\n\n  return mergejoin, whenMatchedUpdateset, whenNotMatchedUpdateset"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d1ce9873-6dc4-449f-9f92-6ed204d82813"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def report_Measure_calculation(stepdf, calc_level, reports_input):  \n  df_report = spark.read.format(\"csv\").option(\"header\", True).option(\"inferSchema\",True).option(\"delimiter\",\",\").load(reports_input).na.fill(0).alias(\"df_report\")  \n  \n  df_report_measured_val = df_report.alias(\"reports\").join(stepdf.alias(\"source\"), [F.col(\"reports.CalcName\") == F.col(\"source.CalcName\")], 'left' )\n  df_report_measured_val = df_report_measured_val.selectExpr(\"ID\", \"PlantID\",\"reports.CalcName\",  \"Calculation\", \"case when reports.CalcValue = 0 then source.CalcValue else reports.CalcValue end as CalcValue\", \"CalcDesc\" ).na.fill(0)\n  \n  casestr =\"\"\n  finalstr=\"\"\n  for i in range(0, calc_level):\n    df_report_pivot = df_report_measured_val\\\n        .groupby(F.col(\"PlantID\"))\\\n        .pivot(\"CalcName\")\\\n        .agg(F.avg(\"CalcValue\"))\n    df_report_measured = df_report_measured_val.join(df_report_pivot, [\"PlantID\"] ).orderBy(\"id\") \n    caserdd = df_report_measured.rdd.collect()\n\n    for i in range(0,  len(caserdd)):\n        casestr =   \" when CalcName = '\" + str(caserdd[i][\"CalcName\"]) + \"' then coalesce(nvl(\" + str(caserdd[i][\"Calculation\"]) + \" , CalcValue),0)\" \n        finalstr = finalstr + casestr\n    df_report_measured = df_report_measured.selectExpr(\"*\",\"\"\"case \"\"\"+ finalstr.replace(\"None\", \"Null\")+\"\"\"  else `CalcValue` end as `finaValue`\"\"\") \n    df_report_measured = df_report_measured.fillna(0).withColumn(\"CalcValue\", F.col(\"finaValue\")).select(df_report_measured_val.columns)\n    df_report_measured = df_report_measured.withColumn(\"ReportDateID\", F.date_format(F.lit(ReportDate).cast(\"string\"),'yyyyMMdd'))\n  df_report_measured = df_report_measured.where(\"CalcName is not null\").selectExpr(\"PlantID\", \"ReportDateID\",\"'' as CalcCategory\", \"'Daily' as CalcFrequency\",\"CalcName\", \"cast(CalcValue as string)\", \"'Null' as UoM\", \"CalcDesc\", \"'Null' as TextInput\")\n  df_report_measured.registerTempTable(\"tempreport\")\n  return True\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0b0083b3-157f-4f9c-a4c2-0535c3fbd839"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["def parse_array_from_string(x):\n    res = json.loads(x)\n    return res\n\nretrieve_array = F.udf(parse_array_from_string, T.ArrayType(T.MapType(T.StringType(),T.StringType())))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bc001057-c66e-4183-bfb3-a74059753961"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["def pre_stage_formconfig(df_incoming, tableAppend,tableName, dateFormat, tsFormat, filename=\"false\"):\n  df_incoming = getalias(df_incoming)\n  no_stage_table='N'\n\n  if replace_char !='N':\n    df_incoming = replaceChar(replace_char, df_incoming)     \n\n  if len(out_partition)!=0:  \n    df_incoming,stg_partition = add_partition(out_partition,df_incoming)\n  else:\n    stg_partition=[]\n  try :\n    df_existing_stage = spark.sql(\"SELECT * FROM deltastage.\"+tableName)\n    schema=df_existing_stage.schema\n  except:\n    no_stage_table='Y'\n    schema=df_incoming.schema    \n    \n  df_incoming=df_incoming.withColumn(\"column1\", retrieve_array(F.col(\"columns\"))).select(\"*\", explode(\"column1\").alias(\"col\"))\n  df_incoming=df_incoming.withColumn(\"name\", F.col(\"col.name\"))\\\n                          .withColumn(\"index\", F.col(\"col.index\"))\\\n                          .withColumn(\"Coluuid\", F.col(\"col.uuid\"))\n  \n  df_prestage=convert_datatype(schema,df_incoming,tableAppend=tableAppend,tableName=tableName,dateFormat=dateFormat, tsFormat=tsFormat,no_stage_table=no_stage_table)\n\n  df_prestage = add_missing_columns(schema, df_prestage)\n  #This is to replace extra spaces from table name mostly for Excel sheet names\n  tableName=tableName.replace(\" \", \"\")\n  if len(stg_partition) != 0:  \n    df_prestage.write.format(\"delta\").mode(\"overwrite\").partitionBy(stg_partition).option(\"path\",stage).\\\n            option(\"overwriteSchema\", True).saveAsTable(\"deltastage.\"+tableName.replace(\" \",\"\"))\n  else:\n    df_prestage.write.format(\"delta\").mode(\"overwrite\").option(\"path\",stage).saveAsTable(\"deltastage.\"+tableName.replace(\" \",\"\"))\n  \n  spark.sql(\"refresh table deltastage.`\" + tableName.replace(\" \",\"\")+ \"`\")\n\n  return df_prestage"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6e46e7ed-333a-49b9-bd4a-1c4a3b008890"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Functions_delta","dashboards":[],"language":"python","widgets":{},"notebookOrigID":1005556096473943}},"nbformat":4,"nbformat_minor":0}
