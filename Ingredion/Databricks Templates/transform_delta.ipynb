{"cells":[{"cell_type":"code","source":["\"\"\"\nDec 01, 2020: When sqlrefresh true then newly added 'sqlrefresh_query' parameter can refresh only 7 days prior data from SQL server. \nNote: If sqlrefresh_query = \"\" or sqlrefresh_query is not provided in inputjson then entire SQL table will be refreshed into ADLS.\n\nDec02, 2020: when sqlrefresh true/false save SQL table in temporary ADLS table.\noverwrite will directly overwrite ADLS and SQL data\nAppend: SQL refresh false: Incoming data will append to ADLS and SQL data\nAppend: SQL refresh true: SQL data will merge into ADLS and then incoming data will merge into ADLS and SQL\nupsert: if sqlrefresh is true or false then SQL table data will be loaded into temp table.\n  sqlrefresh: false: incoming data will merge into ADLS and then merge into internal temp(sql refresh data) with name outputDB.outputTable+\"_temp\". Temp table extract will load into sql\n  sqlrefresh: true: incoming data will merge into ADLS and then merge into temp(sql refresh data). Temp table extract will load into sql. After this Temp table extract will merge into ADLS table\n  - Temp table will be dropped once load completed.\nthen merge final sql table data into ADLS\n#Dec, 12, 2020: 1. sqlSchema parameter indicates schema name of the table. if not provided or blank then default \"dbo\" schema will be used. \n                2. truncate parameter is added for truncate in sql table write. It is added incase of overwrite and if truncate option should be 'False'.\n*****\n\"\"\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ed34fd94-e628-460e-a023-1d05cac57566"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[41]: &#39;\\nDec 01, 2020: When sqlrefresh true then newly added \\&#39;sqlrefresh_query\\&#39; parameter can refresh only 7 days prior data from SQL server. \\nNote: If sqlrefresh_query = &#34;&#34; or sqlrefresh_query is not provided in inputjson then entire SQL table will be refreshed into ADLS.\\n\\nDec02, 2020: when sqlrefresh true/false save SQL table in temporary ADLS table.\\noverwrite will directly overwrite ADLS and SQL data\\nAppend: SQL refresh false: Incoming data will append to ADLS and SQL data\\nAppend: SQL refresh true: SQL data will merge into ADLS and then incoming data will merge into ADLS and SQL\\nupsert: if sqlrefresh is true or false then SQL table data will be loaded into temp table.\\n  sqlrefresh: false: incoming data will merge into ADLS and then merge into internal temp(sql refresh data) with name outputDB.outputTable+&#34;_temp&#34;. Temp table extract will load into sql\\n  sqlrefresh: true: incoming data will merge into ADLS and then merge into temp(sql refresh data). Temp table extract will load into sql. After this Temp table extract will merge into ADLS table\\n  - Temp table will be dropped once load completed.\\nthen merge final sql table data into ADLS\\n#Dec, 12, 2020: 1. sqlSchema parameter indicates schema name of the table. if not provided or blank then default &#34;dbo&#34; schema will be used. \\n                2. truncate parameter is added for truncate in sql table write. It is added incase of overwrite and if truncate option should be \\&#39;False\\&#39;.\\n*****\\n&#39;</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[41]: &#39;\\nDec 01, 2020: When sqlrefresh true then newly added \\&#39;sqlrefresh_query\\&#39; parameter can refresh only 7 days prior data from SQL server. \\nNote: If sqlrefresh_query = &#34;&#34; or sqlrefresh_query is not provided in inputjson then entire SQL table will be refreshed into ADLS.\\n\\nDec02, 2020: when sqlrefresh true/false save SQL table in temporary ADLS table.\\noverwrite will directly overwrite ADLS and SQL data\\nAppend: SQL refresh false: Incoming data will append to ADLS and SQL data\\nAppend: SQL refresh true: SQL data will merge into ADLS and then incoming data will merge into ADLS and SQL\\nupsert: if sqlrefresh is true or false then SQL table data will be loaded into temp table.\\n  sqlrefresh: false: incoming data will merge into ADLS and then merge into internal temp(sql refresh data) with name outputDB.outputTable+&#34;_temp&#34;. Temp table extract will load into sql\\n  sqlrefresh: true: incoming data will merge into ADLS and then merge into temp(sql refresh data). Temp table extract will load into sql. After this Temp table extract will merge into ADLS table\\n  - Temp table will be dropped once load completed.\\nthen merge final sql table data into ADLS\\n#Dec, 12, 2020: 1. sqlSchema parameter indicates schema name of the table. if not provided or blank then default &#34;dbo&#34; schema will be used. \\n                2. truncate parameter is added for truncate in sql table write. It is added incase of overwrite and if truncate option should be \\&#39;False\\&#39;.\\n*****\\n&#39;</div>"]}}],"execution_count":0},{"cell_type":"code","source":["##This is to laod all required libraries\nfrom functools import reduce\nfrom typing import Dict, List, Any\nfrom datetime import datetime\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.types import ArrayType, StringType, MapType, StructField, StructType, FloatType\nfrom pyspark.sql import *\nfrom delta.tables import *\nimport pkg_resources\nimport os, glob \nimport pyspark.sql.functions as F\nimport pyspark.sql.types as T\nimport importlib\nimport logging\nimport pkgutil\nimport json\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"999d0651-0da4-488e-92e4-62af26733737"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%run ../functions/Functions_delta"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f78fdd70-35a2-4bc8-87a9-319c0b3ce07a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["try:\n  trans_name\nexcept:\n  trans_name =\"\"\ninput_file =dbutils.widgets.get(\"input_file\")\n#get input parameters\nwith open(input_file, 'r') as file:\n  data = file.read()\njsonObject = json.loads(data)\nif (trans_name == \"jointransform\"):  \n  inputs: Dict[str, Dict[str, Any]] = jsonObject[\"jointransform\"]\nelse:\n  inputs: Dict[str, Dict[str, Any]] = jsonObject[\"transform\"]\n\ntry:\n  sort_key = inputs[\"sort_key\"] # mandatory parameter for getting latest records only. avoids duplicate records\n  if len(sort_key) == 0 or sort_key ==\"\":\n    sort_key = ['DateInserted']\nexcept:\n  sort_key = ['DateInserted']\nloadstrategy = inputs[\"loadstrategy\"]\n\n# business_key parameter is not mandatory for loadstrategies APPEND and OVERWRITE, unless you want to remove duplicates in the same load. For UPSERT, business_key is mandatory.\ntry:\n  business_key= inputs[\"bussiness_keys\"]\n  if business_key==[]:\n    business_key=\"\"\nexcept:\n  business_key = \"\"\n\n# Add if extract needs to be partitioned\ntry:\n    extractPartition = inputs[\"extractPartition\"]\nexcept: \n    extractPartition = []  \n\n# Add if extract needs to have a specific delimiter\ntry:\n  extractDelimiter = inputs['extractDelimiter']\nexcept:\n  extractDelimiter = ','\n\n# If flag copytoTemp is true, then data to be dumped to SQL would be written to a temp location.\ntry:\n  if inputs['copytoTemp'].lower() == 'true':\n    copytoTemp = True\n  else:\n    copytoTemp = False\nexcept: \n  copytoTemp = False\n\nnon_align_columns=inputs[\"non_align_columns\"]\n \n\ninputTable=inputs[\"inputTable\"]\noutputDB=inputs[\"outputDB\"] # database name 'transform' for transform notebook, 'datamart' for join notebook\noutputTable=inputs[\"outputTable\"]\noutputExtract=inputs[\"outputExtract\"] #save as table or xml\n  \ntry: \n  remove_keys = inputs['remove_keys'] #adding identity column\nexcept:\n  remove_keys = \"\"\n  \n## This is only applied to Mongo data coming for Neutrinos, should not be used for any other use case\ntry: \n  unpivot = inputs[\"unpivot\"] #separating try except blocks\nexcept: \n  unpivot = 'False'\n   \ntry:\n  outputFileformat=inputs[\"outputFileformat\"] #separating try except blocks\nexcept:\n  outputFileformat = \"\" \n   \nadlsPath=inputs[\"adlsPath\"]\nadlsfolder=inputs[\"adlsfolder\"] #folder inside tranform container on adls eg. sap/\nhour_partition=inputs[\"hour_partition\"]\ndate_column= \"N\" if not inputs[\"date_column\"].strip() else inputs[\"date_column\"].strip()\n\ntry:\n  dateFormat = inputs[\"dateFormat\"].strip() #adding dateFormat variable if not present in json input \nexcept:\n  dateFormat = \"yyyy-MM-dd\"\n\n##This variable is timestamp format you want to use \ntry:\n  tsFormat = inputs[\"tsFormat\"] #adding tsFormat variable \nexcept:\n  tsFormat = \"\"\n\nxmlpath= \"/mnt/\"+outputDB+\"/\" + adlsfolder+outputTable +\"_xml\" #path where xml file will be saved\ntry:\n  repartition=\"False\" if not inputs[\"repartition\"].strip() else inputs[\"repartition\"].strip()\nexcept:\n  repartition=\"False\"\n "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ffcc9027-39b5-4c89-bc83-823059a70a80"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{"input_file":"/dbfs/mnt/deltajobinputs/sap/PROS/PurchasedMaterial"}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">KeyError</span>                                  Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-884048354113908&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     11</span>   inputs<span class=\"ansi-blue-fg\">:</span> Dict<span class=\"ansi-blue-fg\">[</span>str<span class=\"ansi-blue-fg\">,</span> Dict<span class=\"ansi-blue-fg\">[</span>str<span class=\"ansi-blue-fg\">,</span> Any<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">=</span> jsonObject<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#34;jointransform&#34;</span><span class=\"ansi-blue-fg\">]</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     12</span> <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 13</span><span class=\"ansi-red-fg\">   </span>inputs<span class=\"ansi-blue-fg\">:</span> Dict<span class=\"ansi-blue-fg\">[</span>str<span class=\"ansi-blue-fg\">,</span> Dict<span class=\"ansi-blue-fg\">[</span>str<span class=\"ansi-blue-fg\">,</span> Any<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">=</span> jsonObject<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#34;transform&#34;</span><span class=\"ansi-blue-fg\">]</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     14</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">     15</span> <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-red-fg\">KeyError</span>: &#39;transform&#39;</div>","errorSummary":"<span class=\"ansi-red-fg\">KeyError</span>: &#39;transform&#39;","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">KeyError</span>                                  Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-884048354113908&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     11</span>   inputs<span class=\"ansi-blue-fg\">:</span> Dict<span class=\"ansi-blue-fg\">[</span>str<span class=\"ansi-blue-fg\">,</span> Dict<span class=\"ansi-blue-fg\">[</span>str<span class=\"ansi-blue-fg\">,</span> Any<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">=</span> jsonObject<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#34;jointransform&#34;</span><span class=\"ansi-blue-fg\">]</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     12</span> <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 13</span><span class=\"ansi-red-fg\">   </span>inputs<span class=\"ansi-blue-fg\">:</span> Dict<span class=\"ansi-blue-fg\">[</span>str<span class=\"ansi-blue-fg\">,</span> Dict<span class=\"ansi-blue-fg\">[</span>str<span class=\"ansi-blue-fg\">,</span> Any<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">=</span> jsonObject<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#34;transform&#34;</span><span class=\"ansi-blue-fg\">]</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     14</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">     15</span> <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-red-fg\">KeyError</span>: &#39;transform&#39;</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# stage and transform_path variables stores value for stage and transform/datamart(outputDB) ADLS location\nstage = \"abfss://deltastage@\"+adlsPath + \"/\"+inputTable\ntransform_path = \"abfss://\"+outputDB+\"@\"+adlsPath+ \"/\"+ adlsfolder+ outputTable\n_timestamp:datetime = datetime.now()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"78fce920-f680-4d66-ac4a-7017ba46cba0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-884048354113909&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> <span class=\"ansi-red-fg\"># stage and transform_path variables stores value for stage and transform/datamart(outputDB) ADLS location</span>\n<span class=\"ansi-green-fg\">----&gt; 2</span><span class=\"ansi-red-fg\"> </span>stage <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">&#34;abfss://deltastage@&#34;</span><span class=\"ansi-blue-fg\">+</span>adlsPath <span class=\"ansi-blue-fg\">+</span> <span class=\"ansi-blue-fg\">&#34;/&#34;</span><span class=\"ansi-blue-fg\">+</span>inputTable\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> transform_path <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">&#34;abfss://&#34;</span><span class=\"ansi-blue-fg\">+</span>outputDB<span class=\"ansi-blue-fg\">+</span><span class=\"ansi-blue-fg\">&#34;@&#34;</span><span class=\"ansi-blue-fg\">+</span>adlsPath<span class=\"ansi-blue-fg\">+</span> <span class=\"ansi-blue-fg\">&#34;/&#34;</span><span class=\"ansi-blue-fg\">+</span> adlsfolder<span class=\"ansi-blue-fg\">+</span> outputTable\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> _timestamp<span class=\"ansi-blue-fg\">:</span>datetime <span class=\"ansi-blue-fg\">=</span> datetime<span class=\"ansi-blue-fg\">.</span>now<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">NameError</span>: name &#39;adlsPath&#39; is not defined</div>","errorSummary":"<span class=\"ansi-red-fg\">NameError</span>: name &#39;adlsPath&#39; is not defined","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-884048354113909&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> <span class=\"ansi-red-fg\"># stage and transform_path variables stores value for stage and transform/datamart(outputDB) ADLS location</span>\n<span class=\"ansi-green-fg\">----&gt; 2</span><span class=\"ansi-red-fg\"> </span>stage <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">&#34;abfss://deltastage@&#34;</span><span class=\"ansi-blue-fg\">+</span>adlsPath <span class=\"ansi-blue-fg\">+</span> <span class=\"ansi-blue-fg\">&#34;/&#34;</span><span class=\"ansi-blue-fg\">+</span>inputTable\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> transform_path <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">&#34;abfss://&#34;</span><span class=\"ansi-blue-fg\">+</span>outputDB<span class=\"ansi-blue-fg\">+</span><span class=\"ansi-blue-fg\">&#34;@&#34;</span><span class=\"ansi-blue-fg\">+</span>adlsPath<span class=\"ansi-blue-fg\">+</span> <span class=\"ansi-blue-fg\">&#34;/&#34;</span><span class=\"ansi-blue-fg\">+</span> adlsfolder<span class=\"ansi-blue-fg\">+</span> outputTable\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> _timestamp<span class=\"ansi-blue-fg\">:</span>datetime <span class=\"ansi-blue-fg\">=</span> datetime<span class=\"ansi-blue-fg\">.</span>now<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">NameError</span>: name &#39;adlsPath&#39; is not defined</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# If partition_keys are not in input json then partition_keys will be taken from inputTable if on single table is used as source table.\npartition_keys =[]\npartition_keys =inputs[\"partition_keys\"]\nif len(partition_keys)==0:\n  try:\n    targetPartition = spark.sql(\"show partitions deltastage.\"+ inputTable).collect()\n    targetPartitions = targetPartition[0][\"partition\"]\n    partitions1=targetPartitions.split(\"/\")  \n    for column in partitions1:\n      partition_keys.append(column.split(\"=\")[0])\n  except:\n    print(\"No partitions\")\n    \nif hour_partition == \"True\":\n      partition_keys.append(\"hour\")\nprint(partition_keys)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Get output partition keys from stage table","showTitle":true,"inputWidgets":{},"nuid":"0983465b-7bb2-4bde-b620-dbaee8d21849"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"Command skipped","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["if outputExtract.lower() == 'sqltable':\n  tsFormat = inputs[\"tsFormat\"]   #timestamp format \n  dateFormat= inputs[\"dateFormat\"]\n  sqlconnectionFile = \"/dbfs/mnt/deltajobinputs/\"+ inputs[\"sqlconnectionFile\"]\n  try:\n    sqlrefresh = inputs[\"sqlrefresh\"]\n  except:\n    sqlrefresh = 'False'\n  try:\n    sqlrefresh_query = inputs[\"sqlrefresh_query\"]\n  except:\n    sqlrefresh_query = \"\"\n  #Dec, 12, 2020: sqlSchema parameter indicates schema name of the table. if not provided or blank then default \"dbo\" schema will be used.  \n  try:    \n    sqlSchema = inputs[\"sqlSchema\"]\n    if sqlSchema == \"\":\n      sqlSchema = \"dbo\"\n  except:\n    sqlSchema = \"dbo\"\n    \n  #get input parameters\n\n  with open(sqlconnectionFile, 'r') as file:\n    data = file.read()\n  connObject = json.loads(data)\n  \n  conn: Dict[str, Dict[str, Any]] = connObject[\"SQLserver\"]  \n\n  jdbcHostname= conn[\"jdbcHostname\"]\n  jdbcPort= conn[\"jdbcPort\"]\n  jdbcDatabase= conn[\"jdbcDatabase\"]\n  connectionProperties= conn[\"connectionProperties\"]\n\n  # Create the JDBC URL without passing in the user and password parameters.\n  jdbcUrl = \"jdbc:sqlserver://{0}:{1};database={2}\".format(jdbcHostname,jdbcPort,jdbcDatabase)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"SQL DB connection","showTitle":true,"inputWidgets":{},"nuid":"028dd833-2a1c-47c5-8e7b-dd83e1950941"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"Command skipped","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":[" # In case of upsert, table should be created first. If table does not exists then it will load same as overwrite. \ntarget =\"\"\ntry: \n  df_existing = spark.sql(\"select * from \"+outputDB+\".\"+ outputTable)\nexcept:\n  target = False  \n  loadstrategy = \"overwrite\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Check ADLS table is existing","showTitle":true,"inputWidgets":{},"nuid":"0b49e509-8d4e-4bcc-b2f8-5311f5d88656"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"Command skipped","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["if outputExtract.lower() == 'sqltable':  \n    try:\n      #Dec 01, 2020: when sqlrefresh is true and if sql refresh is needed only for last 7 days data then sqlrefresh_query input parameter is mandatory.\n      if (sqlrefresh.lower() == 'true'):\n        if (sqlrefresh_query != \"\"):\n          pushdown_query = \"(\"+sqlrefresh_query+\") sqltable\"          \n        else:\n          pushdown_query = \"(select * from \" + sqlSchema +\".\"  + outputTable +\") sqltable\"\n      else:    \n          pushdown_query = \"(select * from \" + sqlSchema +\".\" + outputTable +\") sqltable\"\n      # sql data will be refreshed at the begining and sql_df dataframe will be required while loading data back to the SQL\n      sql_df = spark.read.jdbc(url=jdbcUrl, table=pushdown_query, properties=connectionProperties)\n      sql_schema = sql_df.schema\n    except:\n      print(\"SQL table should be created in SQL server\")\n    \n    \n    if target != False :      \n    #adl_refresh is sql data where partition key columns will be added as Null \n      try:\n        adls_refresh = sql_df\n      except:\n        print(\"sqlrefresh is True but table does not exists in SQL server\")\n\n      adls_refresh = add_datepartition(adls_refresh, date_column, dateFormat) #add_datepartition function will add partition column based on date to dataframe before loading to ADLS as using date_column\n\n      # add partition key columns to dataframe other than date columns which are not available in SQL table\n      datepartitions= [\"date\",\"month\",\"year\"]\n      nondatepartition_keys =partition_keys[:]\n      for dp in datepartitions:\n        nondatepartition_keys.remove(dp)\n      print(nondatepartition_keys)\n      \n      for nk in nondatepartition_keys:\n        if nk not in sql_df.columns:\n          adls_refresh = adls_refresh.withColumn(nk, F.lit('sql'))\n\n      # align sql table columns with existing ADLS table\n      adls_refresh = align_columns(adls_refresh,df_existing,add_missing=True)\n      # Dec 10 2020: change adls_refresh datatypes according to existing tables data types\n      adls_refresh = convert_datatype(df_existing.schema,adls_refresh,tableAppend='N',tableName=outputTable,dateFormat=dateFormat, tsFormat=tsFormat,no_stage_table='N')\n      adls_refresh = adls_refresh.distinct().drop(remove_keys)    \n      \n      #Dec 2, 2020: If append and sqlrefresh true then merge SQL data with ADLS. business_key parameter is mandatory. \n      if loadstrategy.lower() == 'append' and sqlrefresh.lower() == 'true':\n        if type(business_key)==list and len(business_key)!=0:\n          #this function generates required conditions for merge by using dataframe that needs to merge and business keys\n          mergejoin, whenMatchedUpdateset, whenNotMatchedUpdateset = merge_inputs(df_existing,business_key)\n          deltaTable = DeltaTable.forPath(spark, transform_path)\n          deltaTable.alias(\"existing\").merge(\n              adls_refresh.alias(\"incoming\"),\n             mergejoin) \\\n            .whenMatchedUpdate(set = whenMatchedUpdateset ) \\\n            .whenNotMatchedInsert(values = whenNotMatchedUpdateset\n            )\\\n          .execute()\n        else:\n          if partition_keys != []:\n            adls_refresh.write.format(\"delta\").mode(\"append\").partitionBy(partition_keys).option(\"path\", transform_path).saveAsTable(outputDB+\".\"+ outputTable)\n          else:\n            adls_refresh.write.format(\"delta\").mode(\"append\").option(\"path\",  transform_path).saveAsTable(outputDB+\".\"+ outputTable)\n    #Dec2, 2020: if upsert then load SQL data into internal temp delta tables\n    \n    if loadstrategy.lower() == 'upsert':\n      if partition_keys != []:\n        adls_refresh.write.format(\"delta\").mode(\"overwrite\").partitionBy(partition_keys).saveAsTable(outputDB+\".\"+ outputTable + \"_temp\")\n      else:\n        adls_refresh.write.format(\"delta\").mode(\"overwrite\").saveAsTable(outputDB+\".\"+ outputTable + \"_temp\")\n    "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"when sqlrefresh=True then SQL table merge with ADLS Existing Table","showTitle":true,"inputWidgets":{},"nuid":"2cc118ef-ef73-4a55-910c-298532e46ac0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"Command skipped","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# mode is append for append and overwrite for upsert and overwrite in case of SQL extract. ADLS overwrite will have mode overwrite but upsert will be done by merge stametment.\nif loadstrategy.lower() == 'append':\n  mode = 'append'\n  truncate = 'False'\nelse:\n  mode = 'overwrite'\n  truncate = 'True'"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"79492270-a2fe-44c3-8c3d-1875650a2a50"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"Command skipped","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["#This creates temporary tables for sources like CSV, SQLtable, Query, dataframe and adls table. These temporary tables can be used in transformation query. \nsources: Dict[str, Dict[str, Any]] =inputs[\"sources\"]\nif len(sources)!=0:\n  input_s: Dict[str, pyspark.sql.DataFrame] = extract(sources)\n  for key in input_s:\n     input_s[key].registerTempTable(key)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Incoming data","showTitle":true,"inputWidgets":{},"nuid":"34bee32f-8422-490e-b80a-6822e6bbf445"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"Command skipped","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# Input parameter \"query\" is used for fetching incoming data \ndf_incoming = spark.sql(inputs[\"query\"]).alias(\"df_incoming\")\nincomingschema = df_incoming.schema"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e288773d-0d09-416d-a2af-c24faf80e091"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"Command skipped","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["\"\"\"unpivot_fields function is to unpivot incoming data, it takes the 3 input parameters \n# 1. Dataframe which will be unpivoted\n# 2. Column name for Unpivoted Column names\n# 3. Column name for Unpivoted Column values\n# this is for Mongodb-Form_data. \"\"\"\n  \nif unpivot == \"True\":\n    df_incoming = unpivot_fields(df_incoming, inputs[\"unpivot_columns\"][0], inputs[\"unpivot_columns\"][1])\n    df_incoming.registerTempTable(outputTable)    \n    incomingschema=df_incoming.schema"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d9ea1b57-6ccc-4d0d-b7ca-929713f3b297"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"Command skipped","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# if date_column is current_date then partitions will be based on current date\ndf_incoming = add_datepartition(df_incoming, date_column, dateFormat) #add_datepartition function will add partition column based on date to dataframe before loading to ADLS as using date_column \n\nif ((hour_partition == \"True\") & (date_column==\"current_date\")):\n    df_incoming=df_incoming.withColumn(\"hour\", F.hour(F.lit(_timestamp)))\nelif ((hour_partition == \"True\") & (date_column!=\"current_date\")):\n  df_incoming=df_incoming.withColumn(\"hour\", F.hour(F.col(date_column)))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Add Partition Columns to incoming data","showTitle":true,"inputWidgets":{},"nuid":"8244e8a9-65cd-4fcb-aa66-332ba4c23d28"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"Command skipped","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# It adds dateInserted and DateUpdated to incoming data and get latest rows from incoming data. Also, It convert datatype same as existing table.\ndf_incoming = df_incoming.withColumn(\"DateInserted\", F.lit(_timestamp)) \\\n                        .withColumn(\"DateUpdated\", F.lit(_timestamp))\n\ntry:\n  df_incoming = get_latest_row(df_incoming, business_key, sort_key) \nexcept:\n  print(\"business_key and sort_key are required for removing duplicates\")\n\ntry:\n  df_incoming = convert_datatype(df_existing.schema,df_incoming,tableAppend='N',tableName=outputTable,dateFormat=dateFormat, tsFormat=tsFormat,no_stage_table='N')\nexcept:\n  target = False\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"abac3da2-7228-4d5c-a842-0c3129559bf9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"Command skipped","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["\"\"\"\"this function generates required conditions for merge: 1. mergejoin: merge join condition based on business key 2. whenMatchedUpdateset: when business key columns matched then replace existing data values with incoming data values. DateInserted will remain same as existing. 3. when business key columns do not match then insert incoming data to existing table. \"\"\"\n\nif loadstrategy.lower() == 'upsert':  \n  mergejoin, whenMatchedUpdateset, whenNotMatchedUpdateset = merge_inputs(df_existing,business_key)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6c6208d6-3006-4d76-b1cf-e3452c84e22e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"Command skipped","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["if (loadstrategy.lower() == 'upsert') and ((outputExtract.lower() != 'sqltable') or (sqlrefresh.lower() == 'false')):\n  deltaTable = DeltaTable.forPath(spark, transform_path)\n  deltaTable.alias(\"existing\").merge(\n      df_incoming.alias(\"incoming\"),\n     mergejoin) \\\n    .whenMatchedUpdate(set = whenMatchedUpdateset ) \\\n    .whenNotMatchedInsert(values = whenNotMatchedUpdateset\n    )\\\n  .execute()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"ADLS UPSERT","showTitle":true,"inputWidgets":{},"nuid":"b33a2403-5e70-4105-acb9-291dec563709"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"Command skipped","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["#Dec 2, 2020: when SQLtable then upsert incoming data with temp table created from SQL table data\nif (loadstrategy.lower() == 'upsert') and (outputExtract.lower() == 'sqltable') :\n    temp_path = \"dbfs:/user/hive/warehouse/\"+outputDB.lower()+\".db/\"+outputTable.lower() + \"_temp\"\n    \n    deltaTable = DeltaTable.forPath(spark, temp_path)\n    deltaTable.alias(\"existing\").merge(\n      df_incoming.alias(\"incoming\"),\n      mergejoin) \\\n    .whenMatchedUpdate(set = whenMatchedUpdateset ) \\\n    .whenNotMatchedInsert(values = whenNotMatchedUpdateset)\\\n    .execute()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"SQL data temp table upsert","showTitle":true,"inputWidgets":{},"nuid":"d2211e48-da0f-4f18-877f-739286fd335f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"Command skipped","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["if loadstrategy.lower() != 'upsert':\n  if partition_keys != []:\n    df_incoming.write.format(\"delta\").mode(mode).partitionBy(partition_keys).option(\"path\",  transform_path).saveAsTable(outputDB+\".\"+ outputTable)\n  else:\n    df_incoming.write.format(\"delta\").mode(mode).option(\"path\",  transform_path).saveAsTable(outputDB+\".\"+ outputTable)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Overwrite and Append","showTitle":true,"inputWidgets":{},"nuid":"02f2c913-ec1b-49c5-b618-eaa93357feb3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"Command skipped","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["if outputExtract.lower() == 'xml':\n  #Dec 16,2020: Added default values for rootTag and rowTag for xml load\n  try:\n    rootTag = inputs[\"xmltags\"][\"rootTag\"]\n    if (rootTag == \"\") or (inputs[\"xmltags\"] == \"\"):\n      rootTag = 'ROWS'\n  except:\n    rootTag = 'ROWS'\n  try:\n    rowTag = inputs[\"xmltags\"][\"rootTag\"]\n    if (rowTag == \"\") or (inputs[\"xmltags\"] == \"\"):\n      rowTag = 'ROW'\n  except:\n    rowTag = 'ROW'\n  # write to xml file. \n  dumpxml(df_incoming, outputTable, xmlpath, rootTag, rowTag, mode)  \nelif outputExtract.lower() == 'csv':\n  CSV_path = \"abfss://\" + outputDB + \"@\" + adlsPath + \"/\" + adlsfolder + outputTable + \"_CSV\"\n  #transform_path is the folder location on ADLS \n  if repartition == \"True\":\n    df_incoming.repartition(1).write.partitionBy(extractPartition).mode(\"overwrite\").format(\"csv\").option(\"header\", True).option(\"inferschema\", True ).option(\"delimiter\", extractDelimiter).option(\"path\", CSV_path).save()\n  else:\n    df_incoming.write.partitionBy(extractPartition).mode(\"overwrite\").format(\"csv\").option(\"header\", True).option(\"inferschema\", True ).option(\"delimiter\", extractDelimiter).option(\"path\", CSV_path).save()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Write to CSV and XML","showTitle":true,"inputWidgets":{},"nuid":"f045ccfc-c95a-4b7f-bab6-ba7ab362dc67"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"Command skipped","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["if outputExtract.lower() == 'sqltable':\n  # Dec 2, 2020: in case of append and ovewrite incoming data will be overwritten/appended in ADLS and SQL server\n  if loadstrategy.lower() != 'upsert':\n    final_schema = df_incoming.schema\n  else:\n    df_temp_extract = spark.sql(\"select * from \"+ outputDB+\".\"+ outputTable+ \"_temp\")    \n    final_schema = df_temp_extract.schema\n    \n  #sql_df is dataframe created using SQL DB data\n  sql_schema = sql_df.schema\n  \n###################################################################################################################################\n  #if sql db columns and ADLS table columns are small/capital \n  if  final_schema != sql_schema:\n    for sql in sql_schema.fields:\n      for fin in final_schema.fields:\n        if sql.name.lower() == fin.name.lower():\n          # Dec 2, 2020: in case of append and ovewrite incoming data will be overwritten/appended in ADLS and SQL server\n          if loadstrategy.lower() != 'upsert':\n            df_sqldump = df_incoming.withColumn(sql.name, F.col(fin.name))            \n          else:\n            df_sqldump = df_temp_extract.withColumn(sql.name, F.col(fin.name))\n            \n# output dataframe will be aligned to sql table, add columns which are missing. Convert data types of the output dataframe to match with SQL table. \n# get_latest_row function will take latest record and remove business key columns duplicates.\n  df_sqldump = align_columns(df_sqldump,sql_df,add_missing=True)\n  df_sqldump = convert_datatype(sql_schema,df_sqldump,tableAppend='N',tableName=outputTable,dateFormat=dateFormat, tsFormat=tsFormat,no_stage_table='N')\n  df_sqldump= df_sqldump.distinct().drop(remove_keys) \n  # Dec 12, 2020: Get varchar and nvarchar column names and their maximum lengths.    \n  schema_query=\"(select COLUMN_NAME, CHARACTER_MAXIMUM_LENGTH from INFORMATION_SCHEMA.COLUMNS where TABLE_NAME='\"+outputTable+\"' and TABLE_SCHEMA='\"+sqlSchema+\"' and DATA_TYPE in ('nvarchar','varchar')) schematable\"\n  sql_stringSchema = spark.read.jdbc(url=jdbcUrl, table=schema_query, properties=connectionProperties)\n  \n  # Dec 12, 2020: trim the string columns for whitespaces and truncate the value if it is greater than maximum length in SQL DB table\n  for sqlcol in sql_stringSchema.rdd.collect():\n    if sqlcol[0] in df_sqldump.columns:\n      if int(sqlcol[1])!=-1:\n        df_sqldump = df_sqldump.withColumn(sqlcol[0],F.trim(F.col(sqlcol[0])).substr(1,int(sqlcol[1])))    \n  try:\n      df_sqldump = get_latest_row(df_sqldump, business_key, sort_key)\n  except:\n      print(\"WARNING: Duplicate records can be in final SQL data\") \n  \n  # dumpsqlserver fumction loads data to sql server database. mode will be overwrite for upsert and overwrite. Defined in cmd 10\n  #Dec, 12, 2020: sqlSchema parameter indicates schema name of the table. if not provided the default \"dbo\" schema will be used.\n  \n  #Jan, 29, 2020: If flag copytoTemp is true, then data to be dumped to SQL would be written to a temp location.\n  if copytoTemp:\n    temp_path = \"abfss://\"+outputDB+\"@\"+adlsPath+ \"/\"+ adlsfolder+ outputTable + '/Temp'\n    spark.sql(\"drop table if exists \"+ outputDB+ \".\" + \"Temp_\" + outputTable)\n    df_sqldump.write.mode(mode).option(\"path\", temp_path).saveAsTable(outputDB+ \".\" + \"Temp_\" + outputTable)\n  else:\n    dumpsqlserver(df_sqldump,jdbcUrl,sqlSchema +\".\" +outputTable,mode,truncate,connectionProperties)\n\n  #Dec 2, 2020: if SQL refresh is 'True' and  loadstrategy is 'Upsert' then final SQL data will be merge into ADLS table\n  if (target != False)  and ((sqlrefresh.lower() == 'true') and loadstrategy.lower() == 'upsert'):\n    df_temp_extract = convert_datatype(df_existing.schema,df_temp_extract,tableAppend='N',tableName=outputTable,dateFormat=dateFormat, tsFormat=tsFormat,no_stage_table='N')\n    #merge condition will be based on df_existing schema\n    mergejoin, whenMatchedUpdateset, whenNotMatchedUpdateset = merge_inputs(df_existing,business_key)\n    deltaTable = DeltaTable.forPath(spark, transform_path)\n    deltaTable.alias(\"existing\").merge(\n      df_temp_extract.alias(\"incoming\"),\n     mergejoin) \\\n    .whenMatchedUpdate(set = whenMatchedUpdateset ) \\\n    .whenNotMatchedInsert(values = whenNotMatchedUpdateset\n    )\\\n  .execute()\n\n  if loadstrategy.lower() == 'upsert':\n    # drop temp table created from SQL table data.\n    spark.sql(\"drop table \"+ outputDB+\".\"+ outputTable+ \"_temp\")\n    "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"SQL data","showTitle":true,"inputWidgets":{},"nuid":"621c0fd9-500e-4149-8e4d-e16436c82e85"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"Command skipped","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"transform_delta","dashboards":[],"language":"python","widgets":{"input_file":{"nuid":"fdc36eb5-8e83-4e22-9920-4072eb2e33e4","currentValue":"/dbfs/mnt/deltajobinputs/sharepoint/Railcar/SafetyStockMonthlyPSI_2","widgetInfo":{"widgetType":"text","name":"input_file","defaultValue":"","label":null,"options":{"widgetType":"text","validationRegex":null}}}},"notebookOrigID":884048354113869}},"nbformat":4,"nbformat_minor":0}
